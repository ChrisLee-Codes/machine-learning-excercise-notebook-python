{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"plot_gradient_descent.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"7XH_qiyHPPSq","colab_type":"code","colab":{}},"cell_type":"code","source":["%matplotlib inline"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tyfY381cPPSw","colab_type":"text"},"cell_type":"markdown","source":["\n","Gradient descent\n","==================\n","\n","An example demoing gradient descent by creating figures that trace the\n","evolution of the optimizer.\n","\n"]},{"metadata":{"id":"L3ai1CcIPPSy","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","import pylab as pl\n","from scipy import optimize\n","\n","x_min, x_max = -1, 2\n","y_min, y_max = 2.25/3*x_min - .2, 2.25/3*x_max - .2"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TctY_PQTWuK0","colab_type":"code","colab":{}},"cell_type":"code","source":["\"\"\"\n","Cost functions\n","================\n","\n","Example cost functions or objective functions to optimize.\n","\"\"\"\n","import numpy as np\n","\n","###############################################################################\n","# Gaussian functions with varying conditionning\n","\n","def gaussian(x):\n","    return np.exp(-np.sum(x**2))\n","\n","\n","def gaussian_prime(x):\n","    return -2*x*np.exp(-np.sum(x**2))\n","\n","\n","def gaussian_prime_prime(x):\n","    return -2*np.exp(-x**2) + 4*x**2*np.exp(-x**2)\n","\n","\n","def mk_gauss(epsilon, ndim=2):\n","    def f(x):\n","        x = np.asarray(x)\n","        y = x.copy()\n","        y *= np.power(epsilon, np.arange(ndim))\n","        return -gaussian(.5*y) + 1\n","\n","    def f_prime(x):\n","        x = np.asarray(x)\n","        y = x.copy()\n","        scaling = np.power(epsilon, np.arange(ndim))\n","        y *= scaling\n","        return -.5*scaling*gaussian_prime(.5*y)\n","\n","    def hessian(x):\n","        epsilon = .07\n","        x = np.asarray(x)\n","        y = x.copy()\n","        scaling = np.power(epsilon, np.arange(ndim))\n","        y *= .5*scaling\n","        H = -.25*np.ones((ndim, ndim))*gaussian(y)\n","        d = 4*y*y[:, np.newaxis]\n","        d.flat[::ndim+1] += -2\n","        H *= d\n","        return H\n","\n","    return f, f_prime, hessian\n","\n","###############################################################################\n","# Quadratic functions with varying conditionning\n","\n","def mk_quad(epsilon, ndim=2):\n","    def f(x):\n","       x = np.asarray(x)\n","       y = x.copy()\n","       y *= np.power(epsilon, np.arange(ndim))\n","       return .33*np.sum(y**2)\n","\n","    def f_prime(x):\n","       x = np.asarray(x)\n","       y = x.copy()\n","       scaling = np.power(epsilon, np.arange(ndim))\n","       y *= scaling\n","       return .33*2*scaling*y\n","\n","    def hessian(x):\n","       scaling = np.power(epsilon, np.arange(ndim))\n","       return .33*2*np.diag(scaling)\n","\n","    return f, f_prime, hessian\n","\n","\n","###############################################################################\n","# Super ill-conditionned problem: the Rosenbrock function\n","\n","def rosenbrock(x):\n","    y = 4*x\n","    y[0] += 1\n","    y[1:] += 3\n","    return np.sum(.5*(1 - y[:-1])**2 + (y[1:] - y[:-1]**2)**2)\n","\n","\n","def rosenbrock_prime(x):\n","    y = 4*x\n","    y[0] += 1\n","    y[1:] += 3\n","    xm = y[1:-1]\n","    xm_m1 = y[:-2]\n","    xm_p1 = y[2:]\n","    der = np.zeros_like(y)\n","    der[1:-1] = 2*(xm - xm_m1**2) - 4*(xm_p1 - xm**2)*xm - .5*2*(1 - xm)\n","    der[0] = -4*y[0]*(y[1] - y[0]**2) - .5*2*(1 - y[0])\n","    der[-1] = 2*(y[-1] - y[-2]**2)\n","    return 4*der\n","\n","\n","def rosenbrock_hessian_(x):\n","    x, y = x\n","    x = 4*x + 1\n","    y = 4*y + 3\n","    return 4*4*np.array((\n","                    (1 - 4*y + 12*x**2, -4*x),\n","                    (             -4*x,    2),\n","                   ))\n","\n","\n","def rosenbrock_hessian(x):\n","    y = 4*x\n","    y[0] += 1\n","    y[1:] += 3\n","\n","    H = np.diag(-4*y[:-1], 1) - np.diag(4*y[:-1], -1)\n","    diagonal = np.zeros_like(y)\n","    diagonal[0] = 12*y[0]**2 - 4*y[1] + 2*.5\n","    diagonal[-1] = 2\n","    diagonal[1:-1] = 3 + 12*y[1:-1]**2 - 4*y[2:]*.5\n","    H = H + np.diag(diagonal)\n","    return 4*4*H\n","\n","\n","###############################################################################\n","# Helpers to wrap the functions\n","\n","class LoggingFunction(object):\n","\n","    def __init__(self, function, counter=None):\n","        self.function = function\n","        if counter is None:\n","            counter = list()\n","        self.counter = counter\n","        self.all_x_i = list()\n","        self.all_y_i = list()\n","        self.all_f_i = list()\n","        self.counts = list()\n","\n","    def __call__(self, x0):\n","        x_i, y_i = x0[:2]\n","        self.all_x_i.append(x_i)\n","        self.all_y_i.append(y_i)\n","        f_i = self.function(np.asarray(x0))\n","        self.all_f_i.append(f_i)\n","        self.counter.append('f')\n","        self.counts.append(len(self.counter))\n","        return f_i\n","\n","class CountingFunction(object):\n","\n","    def __init__(self, function, counter=None):\n","        self.function = function\n","        if counter is None:\n","            counter = list()\n","        self.counter = counter\n","\n","    def __call__(self, x0):\n","        self.counter.append('f_prime')\n","        return self.function(x0)\n","\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"v7yM6IAHPPS3","colab_type":"text"},"cell_type":"markdown","source":["A formatter to print values on contours\n","\n"]},{"metadata":{"id":"GeFoPeegPPS4","colab_type":"code","colab":{}},"cell_type":"code","source":["def super_fmt(value):\n","    if value > 1:\n","        if np.abs(int(value) - value) < .1:\n","            out = '$10^{%.1i}$' % value\n","        else:\n","            out = '$10^{%.1f}$' % value\n","    else:\n","        value = np.exp(value - .01)\n","        if value > .1:\n","            out = '%1.1f' % value\n","        elif value > .01:\n","            out = '%.2f' % value\n","        else:\n","            out = '%.2e' % value\n","    return out"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nG_xylvEPPS-","colab_type":"text"},"cell_type":"markdown","source":["A gradient descent algorithm\n","do not use: its a toy, use scipy's optimize.fmin_cg\n","\n"]},{"metadata":{"id":"pZ5XkHWzPPTA","colab_type":"code","colab":{}},"cell_type":"code","source":["def gradient_descent(x0, f, f_prime, hessian=None, adaptative=False):\n","    x_i, y_i = x0\n","    all_x_i = list()\n","    all_y_i = list()\n","    all_f_i = list()\n","\n","    for i in range(1, 100):\n","        all_x_i.append(x_i)\n","        all_y_i.append(y_i)\n","        all_f_i.append(f([x_i, y_i]))\n","        dx_i, dy_i = f_prime(np.asarray([x_i, y_i]))\n","        if adaptative:\n","            # Compute a step size using a line_search to satisfy the Wolf\n","            # conditions\n","            step = optimize.line_search(f, f_prime,\n","                                np.r_[x_i, y_i], -np.r_[dx_i, dy_i],\n","                                np.r_[dx_i, dy_i], c2=.05)\n","            step = step[0]\n","            if step is None:\n","                step = 0\n","        else:\n","            step = 1\n","        x_i += - step*dx_i\n","        y_i += - step*dy_i\n","        if np.abs(all_f_i[-1]) < 1e-16:\n","            break\n","    return all_x_i, all_y_i, all_f_i\n","\n","\n","def gradient_descent_adaptative(x0, f, f_prime, hessian=None):\n","    return gradient_descent(x0, f, f_prime, adaptative=True)\n","\n","\n","def conjugate_gradient(x0, f, f_prime, hessian=None):\n","    all_x_i = [x0[0]]\n","    all_y_i = [x0[1]]\n","    all_f_i = [f(x0)]\n","    def store(X):\n","        x, y = X\n","        all_x_i.append(x)\n","        all_y_i.append(y)\n","        all_f_i.append(f(X))\n","    optimize.minimize(f, x0, jac=f_prime, method=\"CG\", callback=store, options={\"gtol\": 1e-12})\n","    return all_x_i, all_y_i, all_f_i\n","\n","\n","def newton_cg(x0, f, f_prime, hessian):\n","    all_x_i = [x0[0]]\n","    all_y_i = [x0[1]]\n","    all_f_i = [f(x0)]\n","    def store(X):\n","        x, y = X\n","        all_x_i.append(x)\n","        all_y_i.append(y)\n","        all_f_i.append(f(X))\n","    optimize.minimize(f, x0, method=\"Newton-CG\", jac=f_prime, hess=hessian, callback=store, options={\"xtol\": 1e-12})\n","    return all_x_i, all_y_i, all_f_i\n","\n","\n","def bfgs(x0, f, f_prime, hessian=None):\n","    all_x_i = [x0[0]]\n","    all_y_i = [x0[1]]\n","    all_f_i = [f(x0)]\n","    def store(X):\n","        x, y = X\n","        all_x_i.append(x)\n","        all_y_i.append(y)\n","        all_f_i.append(f(X))\n","    optimize.minimize(f, x0, method=\"BFGS\", jac=f_prime, callback=store, options={\"gtol\": 1e-12})\n","    return all_x_i, all_y_i, all_f_i\n","\n","\n","def powell(x0, f, f_prime, hessian=None):\n","    all_x_i = [x0[0]]\n","    all_y_i = [x0[1]]\n","    all_f_i = [f(x0)]\n","    def store(X):\n","        x, y = X\n","        all_x_i.append(x)\n","        all_y_i.append(y)\n","        all_f_i.append(f(X))\n","    optimize.minimize(f, x0, method=\"Powell\", callback=store, options={\"ftol\": 1e-12})\n","    return all_x_i, all_y_i, all_f_i\n","\n","\n","def nelder_mead(x0, f, f_prime, hessian=None):\n","    all_x_i = [x0[0]]\n","    all_y_i = [x0[1]]\n","    all_f_i = [f(x0)]\n","    def store(X):\n","        x, y = X\n","        all_x_i.append(x)\n","        all_y_i.append(y)\n","        all_f_i.append(f(X))\n","    optimize.minimize(f, x0, method=\"Nelder-Mead\", callback=store, options={\"ftol\": 1e-12})\n","    return all_x_i, all_y_i, all_f_i"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LBTRwCzXPPTE","colab_type":"text"},"cell_type":"markdown","source":["Run different optimizers on these problems\n","\n"]},{"metadata":{"id":"5yGjwjolPPTF","colab_type":"code","colab":{}},"cell_type":"code","source":["levels = dict()\n","\n","for index, ((f, f_prime, hessian), optimizer) in enumerate((\n","                (mk_quad(.7), gradient_descent),\n","                (mk_quad(.7), gradient_descent_adaptative),\n","                (mk_quad(.02), gradient_descent),\n","                (mk_quad(.02), gradient_descent_adaptative),\n","                (mk_gauss(.02), gradient_descent_adaptative),\n","                ((rosenbrock, rosenbrock_prime, rosenbrock_hessian),\n","                                    gradient_descent_adaptative),\n","                (mk_gauss(.02), conjugate_gradient),\n","                ((rosenbrock, rosenbrock_prime, rosenbrock_hessian),\n","                                    conjugate_gradient),\n","                (mk_quad(.02), newton_cg),\n","                (mk_gauss(.02), newton_cg),\n","                ((rosenbrock, rosenbrock_prime, rosenbrock_hessian),\n","                                    newton_cg),\n","                (mk_quad(.02), bfgs),\n","                (mk_gauss(.02), bfgs),\n","                ((rosenbrock, rosenbrock_prime, rosenbrock_hessian),\n","                            bfgs),\n","                (mk_quad(.02), powell),\n","                (mk_gauss(.02), powell),\n","                ((rosenbrock, rosenbrock_prime, rosenbrock_hessian),\n","                            powell),\n","                (mk_gauss(.02), nelder_mead),\n","                ((rosenbrock, rosenbrock_prime, rosenbrock_hessian),\n","                            nelder_mead),\n","            )):\n","\n","    # Compute a gradient-descent\n","    x_i, y_i = 1.6, 1.1\n","    counting_f_prime = CountingFunction(f_prime)\n","    counting_hessian = CountingFunction(hessian)\n","    logging_f = LoggingFunction(f, counter=counting_f_prime.counter)\n","    all_x_i, all_y_i, all_f_i = optimizer(np.array([x_i, y_i]),\n","                                          logging_f, counting_f_prime,\n","                                          hessian=counting_hessian)\n","\n","    # Plot the contour plot\n","    if not max(all_y_i) < y_max:\n","        x_min *= 1.2\n","        x_max *= 1.2\n","        y_min *= 1.2\n","        y_max *= 1.2\n","    x, y = np.mgrid[x_min:x_max:100j, y_min:y_max:100j]\n","    x = x.T\n","    y = y.T\n","\n","    pl.figure(index, figsize=(3, 2.5))\n","    pl.clf()\n","    pl.axes([0, 0, 1, 1])\n","\n","    X = np.concatenate((x[np.newaxis, ...], y[np.newaxis, ...]), axis=0)\n","    z = np.apply_along_axis(f, 0, X)\n","    log_z = np.log(z + .01)\n","    pl.imshow(log_z,\n","            extent=[x_min, x_max, y_min, y_max],\n","            cmap=pl.cm.gray_r, origin='lower',\n","            vmax=log_z.min() + 1.5*log_z.ptp())\n","    contours = pl.contour(log_z,\n","                        levels=levels.get(f, None),\n","                        extent=[x_min, x_max, y_min, y_max],\n","                        cmap=pl.cm.gnuplot, origin='lower')\n","    levels[f] = contours.levels\n","    pl.clabel(contours, inline=1,\n","                fmt=super_fmt, fontsize=14)\n","\n","    pl.plot(all_x_i, all_y_i, 'b-', linewidth=2)\n","    pl.plot(all_x_i, all_y_i, 'k+')\n","\n","    pl.plot(logging_f.all_x_i, logging_f.all_y_i, 'k.', markersize=2)\n","\n","    pl.plot([0], [0], 'rx', markersize=12)\n","\n","\n","    pl.xticks(())\n","    pl.yticks(())\n","    pl.xlim(x_min, x_max)\n","    pl.ylim(y_min, y_max)\n","    pl.draw()\n","\n","    pl.figure(index + 100, figsize=(4, 3))\n","    pl.clf()\n","    pl.semilogy(np.maximum(np.abs(all_f_i), 1e-30), linewidth=2,\n","                label='# iterations')\n","    pl.ylabel('Error on f(x)')\n","    pl.semilogy(logging_f.counts,\n","                np.maximum(np.abs(logging_f.all_f_i), 1e-30),\n","                linewidth=2, color='g', label='# function calls')\n","    pl.legend(loc='upper right', frameon=True, prop=dict(size=11),\n","              borderaxespad=0, handlelength=1.5, handletextpad=.5)\n","    pl.tight_layout()\n","    pl.draw()"],"execution_count":0,"outputs":[]}]}